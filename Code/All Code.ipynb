{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb6670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ccd6df",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "The stock market exhibits very complex temporal dynamics. At times, it seems as though asset prices move spontaneously and randomly. This has motivated the modeling of the stock market as various forms of discrete-time random walks and continuous-time Brownian motions. However, market movements are entirely determined by economic agents who tend to behave in a predictable manner in response to changing economic and idiosyncratic conditions. As a result, models to predict the movements in stock prices have been undertaken by many. Of course, the successful prediction of the stock market has tremendous financial implications. Moreover, well-calibrated predictions would have applications for policymakers in the federal government as far as fiscal policy and the Federal Reserve regarding interest rate adjustments. ARIMA, GARCH, and other traditional time series models have been used towards this end, but in recent years, deep learning methods have grown more popular in order to provide a more flexible approach. The drawback of deep learning methods is that they are overparametrized and require a substantial amount of data for fitting. For financial time series, this poses a problem as we only observe one time series for each asset price. To address this, we will use a recurrent generative adversarial network (rGAN) to generate realistic financial time series from which a recurrent neural network will be trained to predict asset returns. To enhance the performance of our rGAN, we will consider general economic indicators such as unemployment, inflation, and the yield curve. The stock prices we will model are those of Amazon, Apple, Tesla, Google, and JP Morgan from 2012 to 2022. These companies are each among the top 10 in market capitalization currently and provide a diverse basket of sectors. This information was obtained from the Yahoo Finance module in Python while general market information was obtained from the St. Louis Federal Reserve. The economic data has been seasonally adjusted using an ARIMA model. A list of all economic indicators and their description can be found in the appendix.\n",
    "\n",
    "### Previous Work\n",
    "In recent years, GANs have been applied to a myriad of domains with success. With respect to finance however, there has been limited work done toward applying GANs in this context. Takahashi et al. apply generative adversarial networks to stock prices for S&P 500 firms from 1960 to 2018, showing that the generated time series recover the fundamental properties of asset returns. They use a multi-layer perceptron (MLP) and convolutional neural network (CNN) as their generator which does not allow the generator to learn the temporal dependencies present in the time series. Koshiyama et al. demonstrate the ability of conditional generative adversarial networks to simulate realistic data which can then be used for assembling and fine-tuning trading strategies. A shortcoming of these modeling approaches is the failure to account for the economic and political environment in which the market is operating. For example, the unemployment rate and asset returns are negatively correlated on average. If we do not incorporate this information in our generator, then our generated data may not observe properties that we know hold in practice. To ensure properties of the true data distribution hold, a few different approaches have been offered. Goodfellow et al. introduce the notion of feature matching. Rather than maximizing the output of the discriminator, they adjust the objective for the generator so that the statistics of the generated data match those of the true data. This is achieved by penalizing deviations of features of the generated data from those of the true data. These features though are found in the hidden layer of the discriminator, so if the discriminator does not learn the features of interest, then there is no guarantee that the generator will actually learn the properties we desire. Moreover, the GAN objective can be difficult to optimize in practice since training amounts to finding the Nash equilibrium of a two-player minimax game. In adjusting the objective, feature matching can also provide more stability during training to achieve convergence. In a similar vein, Li et al. introduce generative moment matching networks. Using maximum mean discrepancy, their deep generative model is trained by matching all orders of statistics between the generated and observed data.  \n",
    "\n",
    "### Methods\n",
    "Though generative adversarial networks have been shown to recover the fundamental structure of asset returns, using general economic conditions in data generation has not been done. We believe that by incorporating this information, we can create more realistic samples by preserving economic relationships that hold in the markets. In particular, stock prices and interest rates should move inversely, and stock prices should be positively correlated with GDP per capita and negatively correlated with unemployment. If we do not consider these relationships, it is possible that our generated time series do not possess these properties, which is undesirable as we would not be capturing the true data-generating distribution. In this paper, we will use recurrent generative adversarial networks along with general market information to produce synthetic financial time series. To demonstrate that our generated data is similar to the observed data, we will use two methods. First, we will show that the stock returns adhere to the following properties used by Takahashi:\n",
    "\n",
    "(a) $$\\mu \\approx 0 $$ where $\\mu$ is the mean log return\n",
    "\n",
    "(b) linear unpredictability: autocorrelation function for the price return is absent for any $k>1$\n",
    "$$ \\frac{E[(r_t - \\mu)(r_{t+k} - \\mu)]}{\\sigma^2} \\approx 0$$\n",
    "\n",
    "(c) fat-tailed distribution: the tails of the price return distribution follows power-law decay\n",
    "$$ P(r) \\:  \\alpha \\:  r^{-\\alpha} $$ where $\\alpha$ typically ranges from 3 to 5.\n",
    "\n",
    "(d) volatility clustering: large/small price fluctuations tend to cluster together temporally\n",
    "\n",
    "$$ Corr(|r_t|, |r_{t+k}|) \\: \\alpha \\: k^{-\\beta} $$\n",
    "\n",
    "Takahashi also uses the leverage effect and coarse-fine volatility correlation. We will not consider these properties in this paper because the leverage effect is market dependent, so with only five stocks, the true underlying relationship is unclear, and the coarse-fine volatility correlation is very noisy, so with a small amount of data, we cannot reliably estimate this property.\n",
    "\n",
    "In addition to the properties discussed above, we will also check that the relationships with general market conditions hold. Once these properties are demonstrated, we will train two recurrent neural networks (RNNs. One will be trained on the stock prices for the aforementioned companies from the beginning of 2012 to the end of 2021. The other will be trained on the synthetic data created from a recurrent GAN using this data as examples of the true data distribution. These two networks will then be tested on the current year's stock prices to compare performance.\n",
    "\n",
    "If our rGAN learns the data-generating distribution well, then we would expect the predictions to be similar. We will also implement a simple trading strategy and compare the returns generated from the two models to evaluate the performance of our rGAN. \n",
    "\n",
    "In our analysis of stock prices, we will use log returns to measure price movements rather than absolute price movements. We will motivate this by introducing geometric Brownian motion. Geometric Brownian motion is a popular model for stock price movement and is the foundation of the famous Black-Scholes-Merton formula. Geometric Brownian motion is derived from the stochastic differential equation (SDE):\n",
    "\n",
    "$$dS_t = \\mu S_t dt + \\sigma S_t dB_t$$\n",
    "\n",
    "where $B_t$ is standard Brownian motion. Heuristically, we can think of this SDE as \n",
    "\n",
    "$$ \\log(\\frac{S_{t+1}}{S_t}) \\approx \\frac{\\Delta S_t}{S_t} = \\mu \\Delta t + \\sigma Z $$\n",
    "where Z is $N(0,\\Delta t^2)$. This implies then that stock returns are $N(\\mu \\Delta t, \\sigma \\Delta t^2)$. Since we are dealing with the movement of daily stock prices, $\\Delta t$ corresponds to a one day change. As returns depend only on the time difference and not the time itself, returns are stationary with respect to time under this model. This is a desirable property because it means that the statistical properties of returns do not change over time. This is important in this context because implementing our rGAN will require numerous time series as samples from the data-generating distribution. To meet this requirement, we will have to split the time series for each of the five companies. If our time series are not stationary, which would be the case under normal stock prices, then each time series will have different statistical properties, preventing us from learning the data-generating distribution. Fortunately, by using log returns, we avoid this issue and proceed by splitting each time series by the year.  \n",
    "\n",
    "In order to leverage general market conditions to create our rGAN, one issue that arises is economic data is not released daily. Though bonds are actively traded which allows us to have daily interest rates, mortgage rates are released weekly, inflation and unemployment are released monthly, and GDP and debt figures are released quarterly. To interpolate these figures to a daily sclae, we used daily bond interest rates to perform Gaussian process regression. Gaussian process regression assumes $ y\\sim MVN(\\mu, \\Sigma(x)) $ but for our purposes, we assume $\\mu = 0 $. For $\\Sigma(x)$, we use a white kernel along with a squared exponential kernel. In particular,\n",
    "$$ \\Sigma_{i,j}(x) =  \\sigma_n^2 + \\sigma_s^2e^{\\frac{-||x_i - x_j||_2^2}{\\ell^2}}$$ \n",
    "\n",
    "We find $\\sigma_n^2,\\sigma_s^2,\\ell^2$ using maximum likelihood estimation. For new data $(x_p,y_p)$, we can find the conditional predictive distribution, $y_p|x_p,y,x$ using the fact that $(y_p,y)$ are jointly normal along with the properties of the conditional distribution for the multivariate normal distribution. Given the abrupt changes in economic conditions brought on by the COVIV-19 pandemic, a smooth interpolation function induced by the above kernel had difficult in interpolating all 10 years worth of economic data well. To address this, we split our economic data into pre- and post-pandemic data using the date March 1st, 2021. After doing this, Gaussian process regression managed to interpolate the data quite well.\n",
    "\n",
    "Now that these nuances have been settled, we can finally introduce our rGAN. \n",
    "\n",
    "We will model the five stock prices jointly. Modeling each stock separately ignores the correlation present in the returns; moreover, modeling them jointly allows us to borrow information across the five stocks and should help stabilize the learning process.\n",
    "\n",
    "Generative Adversarial Network Likelihood:\n",
    "    \n",
    "$$\\min_{G} \\max_{D} E_{x \\sim p_{data}(x)}[logD(x)] + E_{z \\sim p_{z}(z)}[log(1-D(G(z)))] $$\n",
    "\n",
    "\n",
    "GAN architecture:\n",
    "\n",
    "We will use gated recurrent units (GRUs) for our recurrent neural network architecture. Recurrent neural networks (RNNs) have been shown to suffer from the vanishing gradient problem and struggle to learn long-term dependencies. To address these shortcomings, long short-term memory neural networks (LSTMs) and GRUs have been created. LSTMs use a forget gate, input gate, and output gate to update the hidden state whereas GRUs use only a forget gate and a reset gate. We will use GRUs here because we have a small amount of training data, and GRUs have been shown to outperform LSTMs on small training sets (need reference). Moreover, we will consider using a Leaky RELU activation function as this function has been demonstrated to allow better learning of long-term dependencies (need reference).\n",
    "\n",
    "$$ x_t^u =  RELU(W_{xh}^uh_{t-1}^u + W_{xk}^uy_{t}^u)   $$\n",
    "\n",
    "$$ r_t^u = \\sigma(W_{rh}^uh_{t-1}^u + W_{rk}^uy_{t}^u)  $$\n",
    "\n",
    "$$ m_t^u = tanh(W_{h}(r_t^u\\circ h_{t-1}^u) + W_{input}y_{t}^u)  $$\n",
    "\n",
    "$$ h_t^u = (1-x_t^u)\\circ h_{t-1}^u + x_t^u \\circ m_t^u  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1f6339",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla = yf.Ticker(\"TSLA\")\n",
    "tsla.stock = tsla.history(period=\"11y\")\n",
    "aapl = yf.Ticker(\"AAPL\")\n",
    "aapl.stock = aapl.history(period=\"11y\")\n",
    "jpm = yf.Ticker(\"JPM\")\n",
    "jpm.stock = jpm.history(period=\"11y\")\n",
    "amzn = yf.Ticker(\"AMZN\")\n",
    "amzn.stock = amzn.history(period= \"11y\")\n",
    "goog = yf.Ticker(\"GOOG\")\n",
    "goog.stock = goog.history(period = \"11y\")\n",
    "\n",
    "## daily\n",
    "TBill3 = pd.read_csv(\"3 Month T Bill Rate.csv\")\n",
    "TBill3.columns = [\"Date\", \"TBill3\"]\n",
    "\n",
    "TBill1 = pd.read_csv(\"1 Year T Bill Rate.csv\")\n",
    "TBill1.columns = [\"Date\", \"TBill1\"]\n",
    "\n",
    "TNote10 = pd.read_csv(\"10 Year Treasury Note Rate.csv\")\n",
    "TNote10.columns = [\"Date\", \"TNote10\"]\n",
    "\n",
    "TNote5 = pd.read_csv(\"5 Year Treasury Note Rate.csv\")\n",
    "TNote5.columns = [\"Date\", \"TNote5\"]\n",
    "\n",
    "TBond30 = pd.read_csv(\"30 Year Treasury Bond Rate.csv\")\n",
    "TBond30.columns = [\"Date\", \"TBond30\"]\n",
    "\n",
    "FedFunds = pd.read_csv(\"Effective Federal Funds Rate.csv\")\n",
    "FedFunds.columns = [\"Date\", \"FedFunds\"]\n",
    "\n",
    "## weekly\n",
    "Mortgage30 = pd.read_csv(\"30 Year Mortgage Rate.csv\")\n",
    "Mortgage30.columns = [\"Date\", \"Mortgage30\"]\n",
    "\n",
    "## monthly\n",
    "CPIAll = pd.read_csv(\"CPI All Items.csv\")\n",
    "CPIAll.columns = [\"Date\", \"CPIAll\"]\n",
    "\n",
    "CPIGas = pd.read_csv(\"CPI Gas.csv\")\n",
    "CPIGas.columns = [\"Date\", \"CPIGas\"]\n",
    "\n",
    "Unemployment = pd.read_csv(\"Unemployment Rate.csv\")\n",
    "Unemployment.columns = [\"Date\", \"Unemployment\"]\n",
    "\n",
    "\n",
    "## quarterly\n",
    "GDPCapita = pd.read_csv(\"Real GDP Per Capita.csv\")\n",
    "GDPCapita.columns = [\"Date\", \"GDPCapita\"]\n",
    "\n",
    "PotentialGDP = pd.read_csv(\"Real Potential GDP.csv\")\n",
    "PotentialGDP.columns = [\"Date\", \"PotentialGDP\"]\n",
    "\n",
    "HouseholdDebt = pd.read_csv(\"Household Debt to GDP.csv\")\n",
    "HouseholdDebt.columns = [\"Date\", \"HouseholdDebt\"]\n",
    "\n",
    "GovtDebt = pd.read_csv(\"Public Debt as Pct of GDP.csv\")\n",
    "GovtDebt.columns = [\"Date\", \"GovtDebt\"]\n",
    "\n",
    "daily = pd.merge(FedFunds,TBill3, on = \"Date\", how = \"outer\")\n",
    "daily = pd.merge(daily, TBill1, on = \"Date\", how = \"outer\")\n",
    "daily = pd.merge(daily, TNote5, on = \"Date\", how = \"outer\")\n",
    "daily = pd.merge(daily, TNote10, on = \"Date\", how = \"outer\")\n",
    "daily = pd.merge(daily, TBond30, on = \"Date\", how = \"outer\")\n",
    "weekly = Mortgage30\n",
    "monthly = pd.merge(CPIAll, CPIGas, on = \"Date\", how = \"outer\")\n",
    "monthly = pd.merge(monthly, Unemployment, on = \"Date\", how = \"outer\")\n",
    "quarterly = pd.merge(GDPCapita, PotentialGDP, on = \"Date\", how = \"outer\")\n",
    "quarterly = pd.merge(quarterly, HouseholdDebt, on = \"Date\", how = \"outer\")\n",
    "quarterly = pd.merge(quarterly, GovtDebt, on = \"Date\", how = \"outer\")\n",
    "\n",
    "\n",
    "quarterly[\"GDPCapita\"] = (quarterly[\"GDPCapita\"] - np.mean(quarterly[\"GDPCapita\"]))/np.std(quarterly[\"GDPCapita\"])\n",
    "quarterly[\"PotentialGDP\"] = (quarterly[\"PotentialGDP\"] - np.mean(quarterly[\"PotentialGDP\"]))/np.std(quarterly[\"PotentialGDP\"]) \n",
    "quarterly[\"HouseholdDebt\"] = (quarterly[\"HouseholdDebt\"] - np.mean(quarterly[\"HouseholdDebt\"]))/np.std(quarterly[\"HouseholdDebt\"])\n",
    "quarterly[\"GovtDebt\"] = (quarterly[\"GovtDebt\"] - np.mean(quarterly[\"GovtDebt\"]))/np.std(quarterly[\"GovtDebt\"]) \n",
    "\n",
    "pd.options.display.max_rows = None\n",
    "for j in range(daily.shape[1]):\n",
    "    for i in range(daily.shape[0]):\n",
    "        if daily.iloc[i,j] == \".\":\n",
    "            daily.iloc[i,j] = np.nan\n",
    "\n",
    "for j in range(weekly.shape[1]):\n",
    "    for i in range(weekly.shape[0]):\n",
    "        if weekly.iloc[i,j] == \".\":\n",
    "            weekly.iloc[i,j] = np.nan\n",
    "\n",
    "for j in range(monthly.shape[1]):\n",
    "    for i in range(monthly.shape[0]):\n",
    "        if monthly.iloc[i,j] == \".\":\n",
    "            monthly.iloc[i,j] = np.nan\n",
    "            \n",
    "for j in range(quarterly.shape[1]):\n",
    "    for i in range(quarterly.shape[0]):\n",
    "        if quarterly.iloc[i,j] == \".\":\n",
    "            quarterly.iloc[i,j] = np.nan\n",
    "            \n",
    "daily = daily.astype({\"TBill3\":float,\"TBill1\":float, \"FedFunds\":float, \"TNote5\":float,\"TNote10\":float, \"TBond30\":float})\n",
    "daily[\"Date\"] = pd.to_datetime(daily[\"Date\"])\n",
    "weekly = weekly.astype({\"Mortgage30\":float})\n",
    "weekly[\"Date\"] = pd.to_datetime(weekly[\"Date\"])\n",
    "monthly = monthly.astype({\"Unemployment\":float,\"CPIAll\":float,\"CPIGas\":float})\n",
    "monthly[\"Date\"] = pd.to_datetime(monthly[\"Date\"])\n",
    "quarterly = quarterly.astype({\"GDPCapita\":float,\"PotentialGDP\":float,\"HouseholdDebt\":float,\"GovtDebt\":float})\n",
    "quarterly[\"Date\"] = pd.to_datetime(quarterly[\"Date\"])\n",
    "\n",
    "daily = daily.dropna()\n",
    "daily_weekly = pd.merge(daily,weekly,on = \"Date\", how= \"outer\")\n",
    "\n",
    "daily_weekly_train = daily_weekly.dropna().drop([\"Date\"], axis = 1)\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ExpSineSquared, Matern\n",
    "\n",
    "kernel = WhiteKernel() + 1*RBF()\n",
    "gaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer = 10)\n",
    "gaussian_process.fit(daily_weekly_train.drop([\"Mortgage30\"],axis=1), daily_weekly_train.Mortgage30)\n",
    "\n",
    "mean_prediction, std_prediction = gaussian_process.predict(daily.drop([\"Date\"],axis = 1), return_std=True)\n",
    "\n",
    "daily_weekly[\"Mortgage30_int\"] = mean_prediction\n",
    "\n",
    "sns.relplot(data = daily_weekly, x = \"Date\", y =\"Mortgage30_int\")\n",
    "plt.scatter(weekly[\"Date\"],weekly[\"Mortgage30\"], color=\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0568948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weekly_monthly = pd.merge(daily_weekly[\"Date\"],monthly,on=\"Date\", how = \"outer\").sort_values(by = \"Date\")\n",
    "daily_weekly_monthly = daily_weekly_monthly.set_index([\"Date\"]).shift(1)\n",
    "daily_weekly_monthly.reset_index(inplace = True)\n",
    "daily_weekly_monthly = daily_weekly_monthly.dropna()\n",
    "daily_weekly_monthly = pd.merge(daily_weekly_monthly, daily_weekly, on = \"Date\", how = \"outer\")\n",
    "\n",
    "daily_weekly_monthly_train = daily_weekly_monthly.drop([\"Mortgage30\"], axis = 1).dropna()\n",
    "\n",
    "daily_weekly_monthly_train_pre = daily_weekly_monthly_train.query(\"Date < '2020-03-01'\")\n",
    "daily_weekly_monthly_train_post = daily_weekly_monthly_train.query(\"Date >= '2020-03-01'\")\n",
    "\n",
    "daily_weekly_pre = daily_weekly.query(\"Date < '2020-03-01'\")\n",
    "daily_weekly_post = daily_weekly.query(\"Date >= '2020-03-01'\")\n",
    "monthly_int_pre = pd.DataFrame()\n",
    "monthly_int_post = pd.DataFrame()\n",
    "monthly_variables = [\"Unemployment\", \"CPIAll\", \"CPIGas\"]\n",
    "\n",
    "for i in range(len(monthly_variables)):\n",
    "    kernel_pre = WhiteKernel() + 1*RBF()\n",
    "    gaussian_process_pre = GaussianProcessRegressor(kernel=kernel_pre, n_restarts_optimizer = 10)\n",
    "    gaussian_process_pre.fit(daily_weekly_monthly_train_pre.drop([\"Date\",\"CPIAll\",\"CPIGas\",\"Unemployment\"],axis=1), \n",
    "                     daily_weekly_monthly_train_pre[monthly_variables[i]])\n",
    "    kernel_post = WhiteKernel() + 1*RBF()\n",
    "    gaussian_process_post = GaussianProcessRegressor(kernel=kernel_post, n_restarts_optimizer = 10)\n",
    "    gaussian_process_post.fit(daily_weekly_monthly_train_post.drop([\"Date\",\"CPIAll\",\"CPIGas\",\"Unemployment\"],axis=1), \n",
    "                     daily_weekly_monthly_train_post[monthly_variables[i]])\n",
    "    \n",
    "    mean_prediction_pre, std_prediction_pre = gaussian_process_pre.predict(\n",
    "        daily_weekly_pre.drop([\"Date\",\"Mortgage30\"], axis = 1), return_std=True)\n",
    "    mean_prediction_post, std_prediction_post = gaussian_process_post.predict(\n",
    "        daily_weekly_post.drop([\"Date\", \"Mortgage30\"], axis = 1), return_std = True)\n",
    "    monthly_int_pre[monthly_variables[i] + \"_int\"] = mean_prediction_pre\n",
    "    monthly_int_post[monthly_variables[i] + \"_int\"] = mean_prediction_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_int = pd.concat([monthly_int_pre, monthly_int_post]) \n",
    "monthly_int = monthly_int.reset_index(drop = True)\n",
    "daily_weekly_monthly = pd.concat([daily_weekly, monthly_int], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e74a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data = daily_weekly_monthly, x = \"Date\", y = \"CPIGas_int\")\n",
    "plt.scatter(monthly[\"Date\"],monthly[\"CPIGas\"], color=\"g\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8df9e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.merge(daily_weekly_monthly[\"Date\"], quarterly, on=\"Date\", how = \"outer\").sort_values(by = \"Date\")\n",
    "all_df = all_df.set_index([\"Date\"]).shift(1)\n",
    "all_df.reset_index(inplace = True)\n",
    "all_df = all_df.dropna()\n",
    "all_df = pd.merge(all_df, daily_weekly_monthly, on = \"Date\", how = \"outer\").drop([\"Mortgage30\"], axis=1)\n",
    "\n",
    "all_df[\"GDPCapita\"] = (all_df[\"GDPCapita\"] - np.mean(all_df[\"GDPCapita\"]))/np.std(all_df[\"GDPCapita\"])\n",
    "all_df[\"PotentialGDP\"] = (all_df[\"PotentialGDP\"] - np.mean(all_df[\"PotentialGDP\"]))/np.std(all_df[\"PotentialGDP\"]) \n",
    "all_df[\"HouseholdDebt\"] = (all_df[\"HouseholdDebt\"] - np.mean(all_df[\"HouseholdDebt\"]))/np.std(all_df[\"HouseholdDebt\"])\n",
    "all_df[\"GovtDebt\"] = (all_df[\"GovtDebt\"] - np.mean(all_df[\"GovtDebt\"]))/np.std(all_df[\"GovtDebt\"]) \n",
    "\n",
    "all_df_train = all_df.sort_values(by = \"Date\")\n",
    "all_df_train = all_df_train.dropna()\n",
    "\n",
    "all_df_train_pre = all_df_train.query(\"Date < '2020-03-01'\")\n",
    "all_df_train_post = all_df_train.query(\"Date >= '2020-03-01'\")\n",
    "\n",
    "daily_weekly_monthly_pre = daily_weekly_monthly.query(\"Date < '2020-03-01'\")\n",
    "daily_weekly_monthly_post = daily_weekly_monthly.query(\"Date >= '2020-03-01'\")\n",
    "quarterly_int_pre = pd.DataFrame()\n",
    "quarterly_int_post = pd.DataFrame()\n",
    "quarterly_variables = [\"GDPCapita\", \"PotentialGDP\", \"HouseholdDebt\", \"GovtDebt\"]\n",
    "\n",
    "for i in range(len(quarterly_variables)):\n",
    "    kernel_pre = WhiteKernel() + 1*RBF()\n",
    "    gaussian_process_pre = GaussianProcessRegressor(kernel=kernel_pre, n_restarts_optimizer = 10)\n",
    "    gaussian_process_pre.fit(all_df_train_pre.drop([\"Date\",\"GDPCapita\", \"PotentialGDP\", \"HouseholdDebt\", \"GovtDebt\"],axis=1), \n",
    "                     all_df_train_pre[quarterly_variables[i]])\n",
    "    kernel_post = WhiteKernel() + 1*RBF()\n",
    "    gaussian_process_post = GaussianProcessRegressor(kernel=kernel_post, n_restarts_optimizer = 10)\n",
    "    gaussian_process_post.fit(all_df_train_post.drop([\"Date\",\"GDPCapita\", \"PotentialGDP\", \"HouseholdDebt\", \"GovtDebt\"],axis=1), \n",
    "                     all_df_train_post[quarterly_variables[i]])\n",
    "    \n",
    "    mean_prediction_pre, std_prediction_pre = gaussian_process_pre.predict(\n",
    "        daily_weekly_monthly_pre.drop([\"Date\",\"Mortgage30\"], axis = 1), return_std=True)\n",
    "    mean_prediction_post, std_prediction_post = gaussian_process_post.predict(\n",
    "        daily_weekly_monthly_post.drop([\"Date\", \"Mortgage30\"], axis = 1), return_std = True)\n",
    "    quarterly_int_pre[quarterly_variables[i] + \"_int\"] = mean_prediction_pre\n",
    "    quarterly_int_post[quarterly_variables[i] + \"_int\"] = mean_prediction_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5634a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_int = pd.concat([quarterly_int_pre, quarterly_int_post]) \n",
    "quarterly_int = quarterly_int.reset_index(drop = True)\n",
    "model_df = pd.concat([daily_weekly_monthly, quarterly_int], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc562ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data = model_df, x = \"Date\",y = \"GDPCapita_int\", color=\"g\")\n",
    "plt.scatter(quarterly[\"Date\"], quarterly[\"GDPCapita\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla.stock[\"Prior_Close\"] = tsla.stock.shift(1)[\"Close\"]\n",
    "Log_Daily_Return = np.log(tsla.stock[\"Close\"]/tsla.stock[\"Prior_Close\"])\n",
    "tsla.stock[\"Log_Daily_Return\"] = Log_Daily_Return\n",
    "tsla.stock = tsla.stock.reset_index(\"Date\")\n",
    "\n",
    "aapl.stock[\"Prior_Close\"] = aapl.stock.shift(1)[\"Close\"] \n",
    "Log_Daily_Return = np.log(aapl.stock[\"Close\"]/aapl.stock[\"Prior_Close\"])\n",
    "aapl.stock[\"Log_Daily_Return\"] = Log_Daily_Return\n",
    "aapl.stock = aapl.stock.reset_index(\"Date\")\n",
    "\n",
    "jpm.stock[\"Prior_Close\"] = jpm.stock.shift(1)[\"Close\"] \n",
    "Log_Daily_Return = np.log(jpm.stock[\"Close\"]/jpm.stock[\"Prior_Close\"])\n",
    "jpm.stock[\"Log_Daily_Return\"] = Log_Daily_Return\n",
    "jpm.stock = jpm.stock.reset_index(\"Date\")\n",
    "\n",
    "amzn.stock[\"Prior_Close\"] = amzn.stock.shift(1)[\"Close\"] \n",
    "Log_Daily_Return = np.log(amzn.stock[\"Close\"]/amzn.stock[\"Prior_Close\"])\n",
    "amzn.stock[\"Log_Daily_Return\"] = Log_Daily_Return\n",
    "amzn.stock = amzn.stock.reset_index(\"Date\")\n",
    "\n",
    "goog.stock[\"Prior_Close\"] = goog.stock.shift(1)[\"Close\"] \n",
    "Log_Daily_Return = np.log(goog.stock[\"Close\"]/goog.stock[\"Prior_Close\"])\n",
    "goog.stock[\"Log_Daily_Return\"] = Log_Daily_Return\n",
    "goog.stock = goog.stock.reset_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb7268",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = model_df.drop([\"Mortgage30\"], axis = 1)\n",
    "\n",
    "values = model_df.drop([\"Date\"], axis = 1).iloc[1:].values\n",
    "values2 = model_df.drop([\"Date\"], axis=1).shift(1).dropna().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030857f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.log((values)/(values2))\n",
    "df = pd.DataFrame(values, columns = model_df.columns[1:15])\n",
    "df[\"Date\"] = model_df[\"Date\"]\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606a8e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla.stock = tsla.stock[20:tsla.stock.shape[0]]\n",
    "aapl.stock = aapl.stock[20:aapl.stock.shape[0]]\n",
    "jpm.stock = jpm.stock[20:jpm.stock.shape[0]]\n",
    "amzn.stock = amzn.stock[20:amzn.stock.shape[0]]\n",
    "goog.stock = goog.stock[20:goog.stock.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a30cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tsla.stock[\"Date\"], tsla.stock[\"Log_Daily_Return\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb6193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Date = tsla.stock[\"Date\"]\n",
    "Tsla = tsla.stock[\"Log_Daily_Return\"]\n",
    "Aapl = aapl.stock[\"Log_Daily_Return\"]\n",
    "Jpm = jpm.stock[\"Log_Daily_Return\"]\n",
    "Amzn = amzn.stock[\"Log_Daily_Return\"]\n",
    "Goog = goog.stock[\"Log_Daily_Return\"]\n",
    "Tsla_price = tsla.stock[\"Open\"]\n",
    "Aapl_price = aapl.stock[\"Open\"]\n",
    "Jpm_price = jpm.stock[\"Open\"]\n",
    "Amzn_price = amzn.stock[\"Open\"]\n",
    "Goog_price = goog.stock[\"Open\"]\n",
    "Tsla_volume = tsla.stock[\"Volume\"]\n",
    "Aapl_volume = aapl.stock[\"Volume\"]\n",
    "Jpm_volume = jpm.stock[\"Volume\"]\n",
    "Amzn_volume = amzn.stock[\"Volume\"]\n",
    "Goog_volume = goog.stock[\"Volume\"]\n",
    "\n",
    "\n",
    "financial_data = {\"Date\":Date, \"Tsla\":Tsla, \"Aapl\": Aapl, \"Jpm\": Jpm, \"Amzn\": Amzn, \"Goog\":Goog, \n",
    "                  \"Tsla_price\": Tsla_price, \"Aapl_price\": Aapl_price, \"Jpm_price\": Jpm_price, \"Amzn_price\": Amzn_price, \n",
    "                  \"Goog_price\":Goog_price, \"Tsla_volume\": Tsla_volume, \"Aapl_volume\": Aapl_volume, \"Jpm_volume\": Jpm_volume, \n",
    "                  \"Amzn_volume\": Amzn_volume, \"Goog_volume\":Goog_volume}\n",
    "\n",
    "financial_data = pd.DataFrame(financial_data)\n",
    "\n",
    "## length of economic data is 2711, but 2749 for financial data\n",
    "financial_data = financial_data.reset_index(drop=True)\n",
    "\n",
    "financial_data = financial_data[0:2725]\n",
    "\n",
    "financial_data[\"Date\"] = pd.to_datetime(financial_data[\"Date\"])\n",
    "\n",
    "financial_data[\"Date\"] = pd.to_datetime(financial_data[\"Date\"]).dt.date\n",
    "\n",
    "model_df[\"Date\"] = pd.to_datetime(model_df[\"Date\"]).dt.date\n",
    "\n",
    "data = pd.merge(financial_data, model_df, on = \"Date\", how = \"outer\")\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
    "\n",
    "data_2012 = data.query(\"Date < '2013-01-01'\")\n",
    "data_2013 = data.query(\"Date < '2014-01-01' & Date >= '2013-01-01'\")\n",
    "data_2014 = data.query(\"Date < '2015-01-01' & Date >= '2014-01-01'\")\n",
    "data_2015 = data.query(\"Date < '2016-01-01' & Date >= '2015-01-01'\")\n",
    "data_2016 = data.query(\"Date < '2017-01-01' & Date >= '2016-01-01'\")\n",
    "data_2017 = data.query(\"Date < '2018-01-01' & Date >= '2017-01-01'\")\n",
    "data_2018 = data.query(\"Date < '2019-01-01' & Date >= '2018-01-01'\")\n",
    "data_2019 = data.query(\"Date < '2020-01-01' & Date >= '2019-01-01'\")\n",
    "data_2020 = data.query(\"Date < '2021-01-01' & Date >= '2020-01-01'\")\n",
    "data_2021 = data.query(\"Date < '2022-01-01' & Date >= '2021-01-01'\")\n",
    "data_2022 = data.query(\"Date < '2023-01-01' & Date >= '2022-01-01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed7ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Date\"] = pd.to_datetime(model_df[\"Date\"]).dt.date\n",
    "data2 = pd.merge(financial_data, df, on = \"Date\", how = \"outer\")\n",
    "data2 = data2.dropna()\n",
    "data2[\"Date\"] = pd.to_datetime(data2[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebbc3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = []\n",
    "for i in data.columns[1:6]:\n",
    "    column = data[i]\n",
    "    value = np.round(np.mean(column),4)\n",
    "    mean.append(value)\n",
    "    \n",
    "import powerlaw\n",
    "\n",
    "\n",
    "alpha = []\n",
    "financial_data = financial_data.dropna()\n",
    "for i in data.columns[1:6]:\n",
    "    column = np.abs(data[i])\n",
    "    fitted = powerlaw.Fit(column, verbose = 0)\n",
    "    fitted.alpha = np.round(fitted.alpha,2)\n",
    "    alpha.append(fitted.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee425484",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4980622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_correlation(x,lag = 1):\n",
    "    a = pd.Series(np.reshape(x,(-1)))\n",
    "    b = a.autocorr(lag = lag)\n",
    "    if np.isnan(b) or np.isinf(b):\n",
    "        return 0\n",
    "    return b\n",
    "\n",
    "def acf(x,max_lag=200):\n",
    "    acf = []\n",
    "    for i in range(max_lag):\n",
    "        acf.append(auto_correlation(x,lag=i+1))\n",
    "    return np.array(acf)\n",
    "\n",
    "def acf_abs(x,max_lag=200):\n",
    "    x = np.abs(x)\n",
    "    acf = []\n",
    "    for i in range(max_lag):\n",
    "        acf.append(auto_correlation(x,lag=i+1))\n",
    "    return np.array(acf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfc6b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(1,201))\n",
    "y = acf_abs(generated_data[\"Jpm\"])\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb66191",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(data[\"Jpm_price\"], data[\"Unemployment_int\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38867b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(data2[\"Tsla\"], data2[\"TBond30\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12d26c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Add\n",
    "from keras.optimizers import Adam,Nadam\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Input, Dense, Activation, Reshape,Flatten, Dropout, Lambda, RepeatVector\n",
    "from keras.layers import Add,Multiply\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling1D, Convolution1D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.optimizers import Adam,Nadam\n",
    "from keras.utils import plot_model\n",
    "#from keras.utils.training_utils import multi_gpu_model\n",
    "from keras import backend as K\n",
    "import random\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae7203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxScaler(data):\n",
    "    \"\"\"Min Max normalizer.\n",
    "  \n",
    "  Args:\n",
    "    - data: original data\n",
    "  \n",
    "  Returns:\n",
    "    - norm_data: normalized data\n",
    "  \"\"\"\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    norm_data = numerator / (denominator + 1e-7)\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b74923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_data_loading(data,seq_len):\n",
    "    ori_data = data\n",
    "    temp_data = []    \n",
    "    for i in range(0, 10):\n",
    "        _x = ori_data[i:i + seq_len]\n",
    "        temp_data.append(_x)\n",
    "    idx = np.random.permutation(len(temp_data))    \n",
    "    data = []\n",
    "    for i in range(len(temp_data)):\n",
    "        data.append(temp_data[idx[i]])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229be129",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)\n",
    "X_train = data.iloc[0:2496,6:30].values\n",
    "y_train = data.iloc[0:2496,1:6].values\n",
    "X_test = data.iloc[2496:2705,6:30].values\n",
    "y_test = data.iloc[2496:2705,1:6].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b0f0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time(data):\n",
    "    time = list()\n",
    "    max_seq_len = 0\n",
    "    for i in range(len(data)):\n",
    "        max_seq_len = max(max_seq_len, len(data[i][:,0]))\n",
    "        time.append(len(data[i][:,0]))\n",
    "    \n",
    "    return time, max_seq_len\n",
    "\n",
    "def rnn_cell(module_name, hidden_dim):\n",
    "    if (module_name == 'gru'):\n",
    "        rnn_cell = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim, activation=tf.nn.tanh)\n",
    "    \n",
    "    return rnn_cell\n",
    "\n",
    "\n",
    "def random_generator(batch_size, z_dim, T_mb, max_seq_len):\n",
    "    Z_mb = list()\n",
    "    for i in range(batch_size):\n",
    "        temp = np.zeros([max_seq_len, z_dim])\n",
    "        temp_Z = np.random.uniform(0., 1, [T_mb[i], z_dim])\n",
    "        temp[:T_mb[i],:] = temp_Z\n",
    "        Z_mb.append(temp_Z)\n",
    "    return Z_mb\n",
    "\n",
    "\n",
    "def batch_generator(data, time, batch_size):\n",
    "    no = len(data)\n",
    "    idx = np.random.permutation(no)\n",
    "    train_idx = idx[:batch_size]     \n",
    "    X_mb = list(data[i] for i in train_idx)\n",
    "    T_mb = list(time[i] for i in train_idx)\n",
    "    return X_mb, T_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283c9fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_slim.layers import layers as _layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f64c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timegan (ori_data, parameters):\n",
    "    tf.reset_default_graph()\n",
    "    no, seq_len, dim = np.asarray(ori_data).shape\n",
    "    ori_time, max_seq_len = extract_time(ori_data)\n",
    "    def MinMaxScaler(data):   \n",
    "        min_val = np.min(np.min(data, axis = 0), axis = 0)\n",
    "        data = data - min_val \n",
    "        max_val = np.max(np.max(data, axis = 0), axis = 0)\n",
    "        norm_data = data / (max_val + 1e-7) \n",
    "        return norm_data, min_val, max_val\n",
    "    \n",
    "    ori_data, min_val, max_val = MinMaxScaler(ori_data)\n",
    "              \n",
    "    hidden_dim   = parameters['hidden_dim'] \n",
    "    num_layers   = parameters['num_layer']\n",
    "    iterations   = parameters['iterations']\n",
    "    batch_size   = parameters['batch_size']\n",
    "    module_name  = parameters['module'] \n",
    "    z_dim        = dim\n",
    "    gamma        = 1\n",
    "    \n",
    "    tf.compat.v1.disable_eager_execution() \n",
    "    \n",
    "    X = tf.compat.v1.placeholder(tf.float32, [None, max_seq_len, dim], name = \"myinput_x\")\n",
    "    Z = tf.compat.v1.placeholder(tf.float32, [None, max_seq_len, z_dim], name = \"myinput_z\")\n",
    "    T = tf.compat.v1.placeholder(tf.int32, [None], name = \"myinput_t\")\n",
    "  \n",
    "    def embedder (X, T):\n",
    "        with tf.variable_scope(\"embedder\", reuse = tf.AUTO_REUSE):\n",
    "            e_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name, hidden_dim) for _ in range(num_layers)])\n",
    "            e_outputs, e_last_states = tf.nn.dynamic_rnn(e_cell, X, dtype=tf.float32, sequence_length = T)\n",
    "            H = _layers.fully_connected(e_outputs, hidden_dim, activation_fn=tf.nn.sigmoid)     \n",
    "        return H\n",
    "      \n",
    "    def recovery (H, T):       \n",
    "        with tf.variable_scope(\"recovery\", reuse = tf.AUTO_REUSE):  \n",
    "            print(H)\n",
    "            r_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name, hidden_dim) for _ in range(num_layers)])\n",
    "            r_outputs, r_last_states = tf.nn.dynamic_rnn(r_cell, H, dtype=tf.float32, sequence_length = T)\n",
    "            X_tilde = _layers.fully_connected(r_outputs, dim, activation_fn=tf.nn.sigmoid) \n",
    "        return X_tilde\n",
    "    \n",
    "    def generator (Z, T):  \n",
    "        with tf.variable_scope(\"generator\", reuse = tf.AUTO_REUSE):\n",
    "            e_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name, hidden_dim) for _ in range(num_layers)])\n",
    "            e_outputs, e_last_states = tf.nn.dynamic_rnn(e_cell, Z, dtype=tf.float32, sequence_length = T)\n",
    "            E = _layers.fully_connected(e_outputs, hidden_dim, activation_fn=tf.nn.sigmoid)     \n",
    "        return E\n",
    "      \n",
    "    def supervisor (H, T): \n",
    "        with tf.variable_scope(\"supervisor\", reuse = tf.AUTO_REUSE):\n",
    "            e_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name, hidden_dim) for _ in range(num_layers-1)])\n",
    "            e_outputs, e_last_states = tf.nn.dynamic_rnn(e_cell, H, dtype=tf.float32, sequence_length = T)\n",
    "            S = _layers.fully_connected(e_outputs, hidden_dim, activation_fn=tf.nn.sigmoid)     \n",
    "        return S\n",
    "          \n",
    "    def discriminator (H, T):       \n",
    "        with tf.variable_scope(\"discriminator\", reuse = tf.AUTO_REUSE):\n",
    "            d_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name, hidden_dim) for _ in range(num_layers)])\n",
    "            d_outputs, d_last_states = tf.nn.dynamic_rnn(d_cell, H, dtype=tf.float32, sequence_length = T)\n",
    "            Y_hat = _layers.fully_connected(d_outputs, 1, activation_fn=None) \n",
    "        return Y_hat   \n",
    "    \n",
    "    H = embedder(X, T)\n",
    "    X_tilde = recovery(H, T)\n",
    "    \n",
    "    E_hat = generator(Z, T)\n",
    "    H_hat = supervisor(E_hat, T)\n",
    "    H_hat_supervise = supervisor(H, T)\n",
    "    \n",
    "    X_hat = recovery(H_hat, T)\n",
    "    \n",
    "    Y_fake = discriminator(H_hat, T)\n",
    "    Y_real = discriminator(H, T)     \n",
    "    Y_fake_e = discriminator(E_hat, T)\n",
    "    \n",
    "        \n",
    "    e_vars = [v for v in tf.trainable_variables() if v.name.startswith('embedder')]\n",
    "    r_vars = [v for v in tf.trainable_variables() if v.name.startswith('recovery')]\n",
    "    g_vars = [v for v in tf.trainable_variables() if v.name.startswith('generator')]\n",
    "    s_vars = [v for v in tf.trainable_variables() if v.name.startswith('supervisor')]\n",
    "    d_vars = [v for v in tf.trainable_variables() if v.name.startswith('discriminator')]\n",
    "    \n",
    "  \n",
    "    D_loss_real = tf.losses.sigmoid_cross_entropy(tf.ones_like(Y_real), Y_real)\n",
    "    D_loss_fake = tf.losses.sigmoid_cross_entropy(tf.zeros_like(Y_fake), Y_fake)\n",
    "    D_loss_fake_e = tf.losses.sigmoid_cross_entropy(tf.zeros_like(Y_fake_e), Y_fake_e)\n",
    "    D_loss = D_loss_real + D_loss_fake + gamma * D_loss_fake_e\n",
    "            \n",
    "    G_loss_U = tf.losses.sigmoid_cross_entropy(tf.ones_like(Y_fake), Y_fake)\n",
    "    G_loss_U_e = tf.losses.sigmoid_cross_entropy(tf.ones_like(Y_fake_e), Y_fake_e)\n",
    "    \n",
    " \n",
    "    G_loss_S = tf.losses.mean_squared_error(H[:,1:,:], H_hat_supervise[:,:-1,:])\n",
    "    \n",
    "  \n",
    "    G_loss_V1 = tf.reduce_mean(tf.abs(tf.sqrt(tf.nn.moments(X_hat,[0])[1] + 1e-6) - tf.sqrt(tf.nn.moments(X,[0])[1] + 1e-6)))\n",
    "    G_loss_V2 = tf.reduce_mean(tf.abs((tf.nn.moments(X_hat,[0])[0]) - (tf.nn.moments(X,[0])[0])))\n",
    "    \n",
    "    G_loss_V = G_loss_V1 + G_loss_V2\n",
    "    \n",
    "    G_loss = G_loss_U + gamma * G_loss_U_e + 100 * tf.sqrt(G_loss_S) + 100*G_loss_V \n",
    "            \n",
    "\n",
    "    E_loss_T0 = tf.losses.mean_squared_error(X, X_tilde)\n",
    "    E_loss0 = 10*tf.sqrt(E_loss_T0)\n",
    "    E_loss = E_loss0  + 0.1*G_loss_S\n",
    "    \n",
    "    E0_solver = tf.train.AdamOptimizer().minimize(E_loss0, var_list = e_vars + r_vars)\n",
    "    E_solver = tf.train.AdamOptimizer().minimize(E_loss, var_list = e_vars + r_vars)\n",
    "    D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list = d_vars)\n",
    "    G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list = g_vars + s_vars)      \n",
    "    GS_solver = tf.train.AdamOptimizer().minimize(G_loss_S, var_list = g_vars + s_vars)   \n",
    "        \n",
    "     \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "  \n",
    "    print('Start Embedding Network Training')\n",
    "    \n",
    "    for itt in range(iterations):\n",
    "        X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)           \n",
    "        _, step_e_loss = sess.run([E0_solver, E_loss_T0], feed_dict={X: X_mb, T: T_mb})        \n",
    "        if itt % 100 == 0:\n",
    "            print('step: '+ str(itt) + '/' + str(iterations) + ', e_loss: ' + str(np.round(np.sqrt(step_e_loss),4)) ) \n",
    "      \n",
    "    print('Finish Embedding Network Training')\n",
    "    \n",
    " \n",
    "    print('Start Training with Supervised Loss Only')\n",
    "    \n",
    "    for itt in range(iterations):\n",
    "        X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)      \n",
    "        Z_mb = random_generator(batch_size, z_dim, T_mb, max_seq_len)\n",
    "        _, step_g_loss_s = sess.run([GS_solver, G_loss_S], feed_dict={Z: Z_mb, X: X_mb, T: T_mb})       \n",
    "        if itt % 100 == 0:\n",
    "            print('step: '+ str(itt)  + '/' + str(iterations) +', s_loss: ' + str(np.round(np.sqrt(step_g_loss_s),4)) )\n",
    "      \n",
    "    print('Finish Training with Supervised Loss Only')\n",
    "    \n",
    "    print('Start Joint Training')\n",
    "  \n",
    "    for itt in range(iterations):\n",
    "        for kk in range(2):\n",
    "            X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)               \n",
    "            Z_mb = random_generator(batch_size, z_dim, T_mb, max_seq_len)\n",
    "            _, step_g_loss_u, step_g_loss_s, step_g_loss_v = sess.run(\n",
    "                [G_solver, G_loss_U, G_loss_S, G_loss_V], feed_dict={Z: Z_mb, X: X_mb, T: T_mb})\n",
    "            _, step_e_loss_t0 = sess.run([E_solver, E_loss_T0], feed_dict={Z: Z_mb, X: X_mb, T: T_mb})   \n",
    "           \n",
    "   \n",
    "            X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)           \n",
    "            Z_mb = random_generator(batch_size, z_dim, T_mb, max_seq_len)\n",
    "            check_d_loss = sess.run(D_loss, feed_dict={X: X_mb, T: T_mb, Z: Z_mb})\n",
    "            if (check_d_loss > 0.15):        \n",
    "                _, step_d_loss = sess.run([D_solver, D_loss], feed_dict={X: X_mb, T: T_mb, Z: Z_mb})\n",
    "        \n",
    "    \n",
    "            if itt % 50 == 0:\n",
    "               print('step: '+ str(itt) + '/' + str(iterations) + \n",
    "                     ', d_loss: ' + str(np.round(step_d_loss,4)) + \n",
    "                     ', g_loss_u: ' + str(np.round(step_g_loss_u,4)) + \n",
    "                     ', g_loss_s: ' + str(np.round(np.sqrt(step_g_loss_s),4)) + \n",
    "                     ', g_loss_v: ' + str(np.round(step_g_loss_v,4)) + \n",
    "                     ', e_loss_t0: ' + str(np.round(np.sqrt(step_e_loss_t0),4))  )\n",
    "    \n",
    "    print('Finish Joint Training')\n",
    "    \n",
    "     \n",
    "    Z_mb = random_generator(no, z_dim, ori_time, max_seq_len)\n",
    "    generated_data_curr = sess.run(X_hat, feed_dict={Z: Z_mb, X: ori_data, T: ori_time})    \n",
    "    \n",
    "    generated_data = list()\n",
    "    \n",
    "    for i in range(5*no):\n",
    "        temp = generated_data_curr[i,:ori_time[i],:]\n",
    "        generated_data.append(temp)\n",
    "        \n",
    "    generated_data = generated_data * max_val\n",
    "    generated_data = generated_data + min_val\n",
    "    \n",
    "    return generated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b245783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ori_data = real_data_loading(data.iloc[0:2496,1:25].values,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a94a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_data = []\n",
    "ori_data.append(data_2012.drop([\"Date\"],axis=1).values)\n",
    "ori_data.append(data_2013.drop([\"Date\"],axis=1).values)\n",
    "ori_data.append(data_2014.drop([\"Date\"],axis=1).values)\n",
    "ori_data.append(data_2015.drop([\"Date\"],axis=1).values)\n",
    "ori_data.append(data_2016.drop([\"Date\"],axis=1).values)\n",
    "ori_data.append(data_2017.drop([\"Date\"],axis=1).values)\n",
    "ori_data.append(data_2018.drop([\"Date\"],axis=1).values)\n",
    "ori_data.append(data_2019.drop([\"Date\"],axis=1).values)\n",
    "ori_data.append(data_2020.drop([\"Date\"],axis=1).values)\n",
    "ori_data.append(data_2021.drop([\"Date\"],axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb1c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    ori_data[i] = ori_data[i][0:245,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49950fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3653895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "parameters = dict()  \n",
    "parameters['module'] = 'gru'\n",
    "parameters['hidden_dim'] = 50\n",
    "parameters['num_layer'] = 2\n",
    "parameters['iterations'] = 2000\n",
    "parameters['batch_size'] = 3\n",
    "      \n",
    "generated_data = timegan(ori_data, parameters)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e534f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data2 = generated_data\n",
    "generated_data = generated_data.reshape((-1,24))\n",
    "\n",
    "generated_data = pd.DataFrame(generated_data)\n",
    "\n",
    "generated_data.columns = data.columns[1:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c670bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data.to_csv(r'C:/Users/Owner/Documents/generated_data_GAN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aa556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39f1585",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([data_2012[0:245],data_2013[0:245],data_2014[0:245],data_2015[0:245],data_2016[0:245], \n",
    "                     data_2017[0:245],data_2018[0:245],data_2019[0:245],data_2020[0:245],data_2021[0:245] ],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91f5b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(generated_data[\"Jpm_price\"], generated_data[\"TBond30\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5500d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bond_corr = []\n",
    "gen_bond_corr = []\n",
    "for i in range(5):\n",
    "    bond_corr.append(np.corrcoef(X_train.iloc[:,i+5],X_train[\"TBond30\"])[1,0])\n",
    "    gen_bond_corr.append(np.corrcoef(generated_data.iloc[:,i+4],generated_data[\"TBond30\"])[1,0])\n",
    "                   \n",
    "unemploy_corr = []\n",
    "gen_unemploy_corr = []\n",
    "for i in range(5):\n",
    "    unemploy_corr.append(np.corrcoef(X_train.iloc[:,i+5],X_train[\"Unemployment_int\"])[1,0])\n",
    "    gen_unemploy_corr.append(np.corrcoef(generated_data.iloc[:,i+4],generated_data[\"Unemployment_int\"])[1,0])\n",
    "    \n",
    "gdp_corr = []\n",
    "gen_gdp_corr = []\n",
    "for i in range(5):\n",
    "    gdp_corr.append(np.corrcoef(X_train.iloc[:,i+5],X_train[\"GDPCapita_int\"])[1,0])\n",
    "    gen_gdp_corr.append(np.corrcoef(generated_data.iloc[:,i+4],generated_data[\"GDPCapita_int\"])[1,0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6c46a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(daily[\"Date\"][0:245],generated_data.iloc[0:245,6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedcfd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_mean = []\n",
    "for i in generated_data.columns[0:5]:\n",
    "    column = generated_data[i]\n",
    "    value = np.round(np.mean(column),4)\n",
    "    gen_mean.append(value)\n",
    "\n",
    "mean = []\n",
    "for i in X_train.columns[1:6]:\n",
    "    column = X_train[i]\n",
    "    value = np.round(np.mean(column),4)\n",
    "    mean.append(value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d5f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df = pd.DataFrame([mean, gen_mean, alpha, gen_alpha, bond_corr,gen_bond_corr,\n",
    "                         unemploy_corr,gen_unemploy_corr,gdp_corr,gen_gdp_corr], columns = generated_data.columns[0:5])\n",
    "table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef1059",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_alpha = []\n",
    "for i in range(5):\n",
    "    column = np.abs(generated_data.iloc[:,i])\n",
    "    fitted = powerlaw.Fit(column, verbose = 0)\n",
    "    fitted.alpha = np.round(fitted.alpha,2)\n",
    "    gen_alpha.append(fitted.alpha)\n",
    "    \n",
    "alpha = []\n",
    "for i in range(5):\n",
    "    column = np.abs(X_train.iloc[:,i+1])\n",
    "    fitted = powerlaw.Fit(column, verbose = 0)\n",
    "    fitted.alpha = np.round(fitted.alpha,2)\n",
    "    alpha.append(fitted.alpha)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f15fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(1,201))\n",
    "y = acf_abs(generated_data.iloc[:,3])\n",
    "z = acf_abs(data.iloc[:,4])\n",
    "plt.scatter(x,y)\n",
    "plt.scatter(x,z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5234013",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9100a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = Sequential()\n",
    "model_gru.add(keras.layers.GRU(245, return_sequences=True,input_shape=\n",
    "                               (train.shape[1], train.shape[2])))\n",
    "model_gru.add(keras.layers.GRU(units=150, return_sequences=True))\n",
    "model_gru.add(keras.layers.GRU(units=100))\n",
    "model_gru.add(keras.layers.Dense(units=5))\n",
    "model_gru.compile(loss='mae', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac629378",
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 2000\n",
    "batch_size = 2\n",
    "for iters in range(iters):\n",
    "    for batch in range(batch_size):\n",
    "        sample = random.choice(list(range(10)))\n",
    "        data = ori_data[sample]\n",
    "        train = data[:,5:24]\n",
    "        target = data[:,0:5]\n",
    "        train = train.reshape((train.shape[0],1,train.shape[1]))\n",
    "        target = target.reshape((target.shape[0],1,target.shape[1]))\n",
    "        model_gru.train_on_batch(train,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8a147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru.compile(\n",
    "    loss=\"mae\",\n",
    "    optimizer=\"adam\"\n",
    ")\n",
    "gru_history = model_gru.fit(X_train, y_train, epochs = 250, batch_size=245, validation_data=(X_test, y_test), shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d6686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_history = model_gru.fit(X_train, y_train, epochs = 250, batch_size = 252, \n",
    "                            validation_data=(X_test, y_test), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da2e8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gru_history.history['loss'], label='train')\n",
    "plt.plot(gru_history.history['val_loss'], label='test')\n",
    "plt.title(\"Figure 1\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c5feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "#hide the axes\n",
    "fig.patch.set_visible(False)\n",
    "ax.axis('off')\n",
    "ax.axis('tight')\n",
    "\n",
    "#create data\n",
    "f = pd.DataFrame(np.random.randn(20, 2), columns=['First', 'Second'])\n",
    "\n",
    "#create table\n",
    "table = ax.table(cellText=f.values, colLabels=f.columns, loc='center')\n",
    "\n",
    "#display table\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c752130e",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "An obvious extension to our work would be to collect much more data on all fronts. For simplicity, we used only five stocks for the past 11 years along with 14 economic indicators. Vastly expanding the timeframe, number of stocks, and number of economic indicators would allow better stabilize the learning process and allow us to fit even more flexible models. In addition, we used the generated data as part of a regression problem. For practical purposes, we would like to have some measure of uncertainty in our predictions. Outputting a single number for the predicted stock return provides minimal value. Consequently, to do density estimation in some sense, we could discretize the distribution of log daily returns. We could then treat this as a multi-class classification problem and compare the two outputted vectors of probabilties by using the Kullback-Leibler divergence $D_{KL}(p||q) = \\sum_{k=1}^{n}p_k\\log(\\frac{p_k}{q_k})$. Lastly, it would be interesting to apply reinforcement learning to determine how best to utilize the given predictions to maximize portfolio returns.\n",
    "\n",
    "### References \n",
    "Koshiyama, N. Firoozye, P. Treleaven, Generative Adversarial Networks for Financial Trading Strategies Fine-Tuning and Combination, arXiv preprint arXiv:1901.01751, 2019.\n",
    "\n",
    "Takahashi, S. Chen, Y. Tanaka-Ishii K. Modeling Financial Time Series with Generative Adversarial Networks. Physica A 527 (2019).\n",
    "\n",
    "Salimans Tim, Goodfellow Ian, Zaremba Wojciech, Cheung Vicki, Radford Alec, and Chen Xi. 2016. Improved techniques for training GANs. In Advances in Neural Information Processing Systems. 22342242. \n",
    "\n",
    "Yujia Li, Kevin Swersky, and Richard Zemel. Generative moment matching networks. 10 February 2015.\n",
    "\n",
    "Fekri MN, Ghosh AM, Grolinger K. Generating Energy Data for Machine Learning with Recurrent Generative Adversarial Networks. Energies. 2020; 13(1):130.\n",
    "\n",
    "Cristbal Esteban, Stephanie L Hyland, and Gunnar Rtsch. 2017. Real-valued\n",
    "(medical) time series generation with recurrent conditional GANs. arXiv preprint\n",
    "arXiv:1706.02633 (2017).\n",
    "\n",
    "Homanga Bharadhwaj, Homin Park, and Brian Y. Lim. 2018. RecGAN: recurrent generative adversarial networks for recommendation systems. In Proceedings of the 12th ACM Conference on Recommender Systems. 372-376.\n",
    "\n",
    "Jinsung Yoon, Daniel Jarrett, Mihaela van der Schaar, \"Time-series Generative Adversarial Networks,\" \n",
    "Neural Information Processing Systems (NeurIPS), 2019.\n",
    "\n",
    "### Appendix\n",
    "For general economic indicators, the following were used:\n",
    "\n",
    "Effective Federal Funds Rate: the rate banks charge each other for overnight loans to meet deposit requirments, serves as a measure of policy action by the Federal Reserve\n",
    "\n",
    "3 Month Treasury Bill Rate and 1 Year Treasury Bill Rate: prevailing yield of 3 month and 1 year Treasury bonds; serves as a measure of short term interest rates\n",
    "\n",
    "5 and 10 Year Treasury Bond Rates: prevailing yield of 5 year and 10 year Treasury bonds; serves as a measure of intermediate term interest rates\n",
    "\n",
    "30 Year Treasury Bond Rate: prevailing yield of 30 year Treasury bonds; serves as ameasure of long term interest rates\n",
    "\n",
    "30 Year Mortgage Rate: prevailing average interest rate for a 30 year fixed rate mortgage; measure of long term interest rates as well as quantitative easing for the Federal Reserve and overall health of the housing market \n",
    "\n",
    "Real GDP Per Capita: total economic output divided by the midyear population in terms of chained 2012 dollars; serves as a measure of the economic cycle and overall health of the economy\n",
    "\n",
    "Unemployment Rate: number of people employed divided by the number of people in the labor force; serves as an additional measure of the economic cycle and overall health of the economy\n",
    "\n",
    "Consumer Price Index: price of a basket of goods and services; serves as a measure of inflation\n",
    "\n",
    "Consumer Price Index for Gas: price level of gasoline and related products; serves as a measure of inflation as well as a proxy for foreign policy relations\n",
    "\n",
    "Real Potential GDP: total potential economic capacity; serves as a measure of technological improvement and capital accumulation\n",
    "\n",
    "Public Debt as Percent of GDP: ratio of total outstanding government debt to GDP; captures the spending and taxing policies employed  by the federal government\n",
    "\n",
    "Household Debt as Percent of GDP: ratio of total outstanding household debt to GDP; measures the borrowing habits of consumers and serves as a proxy for consumer confidence and the credit cycle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
