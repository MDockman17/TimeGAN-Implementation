{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d3e74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Add\n",
    "from keras.optimizers import Adam,Nadam\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Input, Dense, Activation, Reshape,Flatten, Dropout, Lambda, RepeatVector\n",
    "from keras.layers import Add,Multiply\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling1D, Convolution1D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.optimizers import Adam,Nadam\n",
    "from keras.utils import plot_model\n",
    "#from keras.utils.training_utils import multi_gpu_model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200edc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model_mlp_bn():\n",
    "    input_noise = Input(shape=(100,))\n",
    "    model = Dense(128,kernel_initializer='random_uniform')(input_noise)\n",
    "    model = BatchNormalization()(model)\n",
    "    #model = LeakyReLU()(model)\n",
    "    model = Activation('tanh')(model)\n",
    "    model = Dense(2048,kernel_initializer='random_uniform')(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('tanh')(model)\n",
    "    model = Dense(8192,kernel_initializer='random_uniform')(model)\n",
    "    #model = BatchNormalization()(model)\n",
    "    model = Activation('tanh')(model)\n",
    "    model = Reshape((8192,1))(model)\n",
    "    from keras.layers import Lambda\n",
    "    def mean_computation(x):\n",
    "        return K.mean(x,axis=1)\n",
    "\n",
    "    def mean_computation_output_shape(input_shape):\n",
    "        new_shape = tuple([input_shape[0],input_shape[-1]])\n",
    "        return new_shape                                          \n",
    "  \n",
    "    def std_computation(x):\n",
    "        return K.std(x,axis=1)\n",
    "\n",
    "    def std_computation_output_shape(input_shape):\n",
    "        new_shape = tuple([input_shape[0],input_shape[-1]])\n",
    "        return new_shape                                          \n",
    "\n",
    "    mean_layer = Lambda(mean_computation,output_shape=mean_computation_output_shape)\n",
    "    std_layer = Lambda(std_computation,output_shape=std_computation_output_shape)\n",
    "    mean = mean_layer(model)\n",
    "    std = std_layer(model)\n",
    "    model = Model(input_noise,model)\n",
    "    model_statistics = Model(input_noise,[mean,std])\n",
    "    return model,model_statistics\n",
    "\n",
    "def generator_model_mlp():\n",
    "    input_noise = Input(shape=(100,))\n",
    "    model = Dense(128,kernel_initializer='random_uniform')(input_noise)\n",
    "    #model = BatchNormalization()(model)\n",
    "    #model = LeakyReLU()(model)\n",
    "    model = Activation('tanh')(model)\n",
    "    model = Dense(2048,kernel_initializer='random_uniform')(model)\n",
    "    #model = BatchNormalization()(model)\n",
    "    model = Activation('tanh')(model)\n",
    "    model = Dense(8192,kernel_initializer='random_uniform')(model)\n",
    "    #model = BatchNormalization()(model)\n",
    "    model = Activation('tanh')(model)\n",
    "    model = Reshape((8192,1))(model)\n",
    "    from keras.layers import Lambda\n",
    "    def mean_computation(x):\n",
    "        return K.mean(x,axis=1)\n",
    "\n",
    "    def mean_computation_output_shape(input_shape):\n",
    "        new_shape = tuple([input_shape[0],input_shape[-1]])\n",
    "        return new_shape                                          \n",
    "  \n",
    "    def std_computation(x):\n",
    "        return K.std(x,axis=1)\n",
    "\n",
    "    def std_computation_output_shape(input_shape):\n",
    "        new_shape = tuple([input_shape[0],input_shape[-1]])\n",
    "        return new_shape                                          \n",
    "\n",
    "    mean_layer = Lambda(mean_computation,output_shape=mean_computation_output_shape)\n",
    "    std_layer = Lambda(std_computation,output_shape=std_computation_output_shape)\n",
    "    mean = mean_layer(model)\n",
    "    std = std_layer(model)\n",
    "    model = Model(input_noise,model)\n",
    "    model_statistics = Model(input_noise,[mean,std])\n",
    "    return model,model_statistics\n",
    "\n",
    "def generator_model_cnn():\n",
    "    input_noise = Input(shape=(100,))\n",
    "    model = Dense(128)(input_noise)\n",
    "    model_1 = Reshape((128,1))(model)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,35,border_mode='same')(model_1)\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,25,border_mode='same')(model_1)\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,15,border_mode='same')(model_1)\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,15,border_mode='same')(model_1)\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,15,border_mode='same')(model_1)\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,15,border_mode='same')(model_1)\n",
    "    model_1_1 = Convolution1D(64,7,border_mode='same')(model_1)\n",
    "    model_1_1_1 = Convolution1D(64,4,border_mode='same')(model_1)\n",
    "    model_1 = Add()([model_1,model_1_1,model_1_1_1])\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = Convolution1D(1,1,border_mode='same')(model_1)\n",
    "    model_1 = Activation('tanh')(model_1)\n",
    "    model = Model(input_noise,model_1)\n",
    "    return model\n",
    "\n",
    "def generator_model_mlp_cnn():\n",
    "    input_noise = Input(shape=(100,))\n",
    "    model = Dense(128)(input_noise)\n",
    "    model_1 = Reshape((128,1))(model)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,35,border_mode='same')(model_1)\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,25,border_mode='same')(model_1)\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,15,border_mode='same')(model_1)\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,15,border_mode='same')(model_1)\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,15,border_mode='same')(model_1)\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,15,border_mode='same')(model_1)\n",
    "    model_1_1 = Convolution1D(64,7,border_mode='same')(model_1)\n",
    "    model_1_1_1 = Convolution1D(64,4,border_mode='same')(model_1)\n",
    "    model_1 = Add()([model_1,model_1_1,model_1_1_1])\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = Convolution1D(1,1,border_mode='same')(model_1)\n",
    "    model_1 = Activation('tanh')(model_1)\n",
    "    model_2 = Dense(8192)(model)\n",
    "    model_2 = Activation('tanh')(model_2)\n",
    "    model_2 = Reshape((8192,1))(model_2)\n",
    "    model = Multiply()([model_1,model_2])\n",
    "    model = Model(input_noise,model)\n",
    "    return model\n",
    "\n",
    "def generator_model_mlp_cnn_plus():\n",
    "    input_noise = Input(shape=(100,))\n",
    "    model = Dense(128)(input_noise)\n",
    "    model_1 = Reshape((128,1))(model)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,35,border_mode='same')(model_1)\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,25,border_mode='same')(model_1)\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,15,border_mode='same')(model_1)\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,15,border_mode='same')(model_1)\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,15,border_mode='same')(model_1)\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,15,border_mode='same')(model_1)\n",
    "    model_1_1 = Convolution1D(64,7,border_mode='same')(model_1)\n",
    "    model_1_1_1 = Convolution1D(64,4,border_mode='same')(model_1)\n",
    "    model_1 = Add()([model_1,model_1_1,model_1_1_1])\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = Convolution1D(1,1,border_mode='same')(model_1)\n",
    "    model_1 = Activation('tanh')(model_1)\n",
    "    model_2 = Dense(8192)(model)\n",
    "    model_2 = Activation('tanh')(model_2)\n",
    "    model_2 = Reshape((8192,1))(model_2)\n",
    "    model_3 = Dense(1)(input_noise)\n",
    "    model_3 = Activation('sigmoid')(model_3)\n",
    "    from keras.layers import RepeatVector\n",
    "    model_3 = RepeatVector(8192)(model_3)\n",
    "    model = Multiply()([model_1,model_2,model_3])\n",
    "    model = Model(input_noise,model)\n",
    "    return model\n",
    "\n",
    "def generator_model_mlp_cnn():\n",
    "    input_noise = Input(shape=(100,))\n",
    "    model = Dense(128)(input_noise)\n",
    "    model_1 = Reshape((128,1))(model)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,35,border_mode='same')(model_1)\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,25,border_mode='same')(model_1)\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,15,border_mode='same')(model_1)\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,15,border_mode='same')(model_1)\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,15,border_mode='same')(model_1)\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = UpSampling1D(2) (model_1)\n",
    "    model_1 = Convolution1D(64,15,border_mode='same')(model_1)\n",
    "    model_1_1 = Convolution1D(64,7,border_mode='same')(model_1)\n",
    "    model_1_1_1 = Convolution1D(64,4,border_mode='same')(model_1)\n",
    "    model_1 = Add()([model_1,model_1_1,model_1_1_1])\n",
    "    model_1 = BatchNormalization()(model_1)\n",
    "    model_1 = LeakyReLU()(model_1)\n",
    "    model_1 = Convolution1D(1,1,border_mode='same')(model_1)\n",
    "    model_1 = Activation('tanh')(model_1)\n",
    "    model_2 = Dense(8192)(model)\n",
    "    model_2 = Activation('tanh')(model_2)\n",
    "    model_2 = Reshape((8192,1))(model_2)\n",
    "    model = Multiply()([model_1,model_2])\n",
    "    from keras.layers import Lambda\n",
    "    def mean_computation(x):\n",
    "        return K.mean(x,axis=1)\n",
    "\n",
    "    def mean_computation_output_shape(input_shape):\n",
    "        new_shape = tuple([input_shape[0],input_shape[-1]])\n",
    "        return new_shape                                          \n",
    "  \n",
    "    def std_computation(x):\n",
    "        return K.std(x,axis=1)\n",
    "\n",
    "    def std_computation_output_shape(input_shape):\n",
    "        new_shape = tuple([input_shape[0],input_shape[-1]])\n",
    "        return new_shape                                          \n",
    "\n",
    "    mean_layer = Lambda(mean_computation,output_shape=mean_computation_output_shape)\n",
    "    std_layer = Lambda(std_computation,output_shape=std_computation_output_shape)\n",
    "    mean = mean_layer(model)\n",
    "    std = std_layer(model)\n",
    "    model = Model(input_noise,model)\n",
    "    model_statistics = Model(input_noise,[mean,std])\n",
    "    return model,model_statistics\n",
    "\n",
    "def discriminator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Convolution1D(64,10,border_mode='same',input_shape=(8192,1)))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Convolution1D(128,10,border_mode='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Convolution1D(128,10,border_mode='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818f4b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def auto_correlation(x,lag = 1):\n",
    "    a = pd.Series(np.reshape(x,(-1)))\n",
    "    b = a.autocorr(lag = lag)\n",
    "    if np.isnan(b) or np.isinf(b):\n",
    "        return 0\n",
    "    return b\n",
    "\n",
    "def acf(x,max_lag=1000):\n",
    "    acf = []\n",
    "    for i in range(max_lag):\n",
    "        acf.append(auto_correlation(x,lag=i+1))\n",
    "    return np.array(acf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dcb978",
   "metadata": {},
   "outputs": [],
   "source": [
    "acf(financial_data[\"Tsla\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17719bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualize as vs\n",
    "import numpy as np\n",
    "import stats\n",
    "\n",
    "#volatility clustering\n",
    "def acf(x,file_name,for_abs=True,multiple=False,fit=False,scale='log',max_lag=1000):\n",
    "    if for_abs:\n",
    "        x = np.abs(x)\n",
    "    if multiple:\n",
    "        res = np.zeros(max_lag)\n",
    "        for e in x:\n",
    "            res += stats.acf(e)\n",
    "        res /= x.shape[0]\n",
    "    else:\n",
    "        res = stats.acf(x)\n",
    "    vs.acf(res,file_name,scale=scale)\n",
    "\n",
    "def leverage_effect(x,file_name,multiple=True,min_lag=1,max_lag=100):\n",
    "    def compute_levs(x,x_abs):\n",
    "        Z = (np.mean(x_abs**2))**2\n",
    "        second_term = np.mean(x)*np.mean(x_abs**2)\n",
    "        def compute_for_t(t):\n",
    "            if t == 0:\n",
    "                first_term = np.mean(x*(x_abs)**2)\n",
    "            elif t > 0:\n",
    "                first_term = np.mean(x[:-t]*(x_abs[t:]**2))\n",
    "            else:\n",
    "                first_term = np.mean(x[-t:]*(x_abs[:t]**2) )\n",
    "            return (first_term-second_term)/Z\n",
    "        levs = [compute_for_t(t) for t in range(min_lag,max_lag)]\n",
    "        return np.array(levs)\n",
    "\n",
    "    x_abs = np.abs(x)\n",
    "    if multiple:\n",
    "        levs = np.zeros(max_lag-min_lag)\n",
    "        for e1,e2 in zip(x,x_abs):\n",
    "            levs += compute_levs(e1,e2)\n",
    "        levs /= x.shape[0]\n",
    "    else:\n",
    "        levs = compute_levs(x,x_abs)\n",
    "    vs.leverage_effect([i for i in range(min_lag,max_lag)],levs,file_name)\n",
    "    return levs\n",
    "\n",
    "\n",
    "#fat-tail\n",
    "def distribution(x,file_name,scale='linear',multiple=False,normalize=True,granuality=100):\n",
    "    #preprocessing\n",
    "    if multiple:\n",
    "        x = np.reshape(x,x.size)\n",
    "    if normalize:\n",
    "        x = normalize_time_series(x)\n",
    "    if scale == 'linear':\n",
    "        dist_x,dist_y = linear_pdf(x,granuality=granuality)\n",
    "        vs.distribution(dist_x, dist_y, file_name, 'linear')\n",
    "        return dist_x, dist_y\n",
    "    elif scale == 'log':\n",
    "        dist_x,dist_y = linear_pdf(x,granuality=granuality)\n",
    "        vs.distribution(dist_x, dist_y, file_name, 'log')\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def culmulative_distribution(x,scale='linear',normalize=True):\n",
    "    pass\n",
    "\n",
    "def normalize_time_series(x):\n",
    "    mean = np.mean(x)\n",
    "    std = np.std(x)\n",
    "    x = (x-mean)/std\n",
    "    return x\n",
    "\n",
    "def linear_pdf(x,dist_x=None,granuality=100):\n",
    "    if dist_x is None:\n",
    "        x_max = 5.\n",
    "        x_min = -5.\n",
    "    dist_x = np.linspace(x_min,x_max,granuality)\n",
    "    diff = dist_x[1]-dist_x[0]\n",
    "    dist_x_visual = (dist_x + diff)[:-1]\n",
    "    dist_y = np.zeros(granuality-1)\n",
    "    for e,(x1,x2) in enumerate(zip(dist_x[:-1],dist_x[1:])):\n",
    "        dist_y[e] = x[np.logical_and(x > x1,x < x2)].size\n",
    "    dist_y /= x.size\n",
    "    return dist_x_visual,dist_y\n",
    "\n",
    "\n",
    "def log_pdf(x,dist_x=None,granuality=100):\n",
    "    pass\n",
    "\n",
    "def cdf(x,scale='linear'):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a0d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    #preparing generator\n",
    "    if args.generator_model == 'mlp-cnn':\n",
    "        generator,generator_statistics = generator_model_mlp_cnn()\n",
    "    elif args.generator_model == 'mlp':\n",
    "        generator,generator_statistics = generator_model_mlp()\n",
    "    elif args.generator_model == 'cnn':\n",
    "        generator = generator_model_cnn()\n",
    "    elif args.generator_model == 'plus':\n",
    "        generator = generator_model_mlp_cnn_plus()\n",
    "    else:\n",
    "        import sys\n",
    "        sys.exit()\n",
    "    #preparing discriminator\n",
    "                                                      \n",
    "    statistics_opt = Adam(lr=0.0001)\n",
    "    generator_statistics.compile(loss='mean_squared_error',optimizer=statistics_opt)\n",
    "\n",
    "    discriminator = discriminator_model()\n",
    "    d_opt = Adam(lr=args.discriminator_lr, beta_1=0.1)\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=d_opt)\n",
    "    discriminator.trainable = False\n",
    "    for e in discriminator.layers:\n",
    "        e.trainable = False\n",
    "    gan = Sequential([generator,discriminator])\n",
    "    g_opt = Adam(lr=args.generator_lr, beta_1=0.5)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=g_opt)\n",
    "    \n",
    "    g_loss_recorder = []\n",
    "    d_loss_recorder = []\n",
    "    g_losses_recorder = []\n",
    "    d_losses_recorder = []\n",
    "    #start training\n",
    "    for epoch in range(args.epochs):\n",
    "        for index in range(args.batches):\n",
    "            noise = np.array([np.random.normal(0,1.0,size=100) for _ in range(batch_size)])\n",
    "            real_series = dg.real_data()\n",
    "            real_series = np.nan_to_num(real_series)\n",
    "            generated_series = generator.predict(noise, verbose=0)\n",
    "            if index % args.log_interval == 0:\n",
    "                sf.acf(generated_series,'./imgs/%s/acf/acf_abs_%i_%i'%(timestamp,epoch,index),for_abs=True)\n",
    "                sf.acf(generated_series,'./imgs/%s/acf/acf_raw_%i_%i'%(timestamp,epoch,index),for_abs=False)\n",
    "                sf.acf(generated_series,'./imgs/%s/acf/acf_abs_linear_%i_%i'%(timestamp,epoch,index),for_abs=True,scale='linear')\n",
    "                sf.acf(generated_series,'./imgs/%s/acf/acf_raw_linear_%i_%i'%(timestamp,epoch,index),for_abs=False,scale='linear')\n",
    "                sf.leverage_effect(generated_series,'./imgs/%s/leverage/leverage_%i_%i'%(timestamp,epoch,index))\n",
    "                sf.distribution(generated_series, './imgs/%s/dist/distribution_%i_%i'%(timestamp,epoch,index),'linear')\n",
    "                sf.distribution(generated_series, './imgs/%s/dist/distribution_%i_%i'%(timestamp,epoch,index),'log')\n",
    "                visualize.time_series(generated_series[0],'./imgs/%s/time_series/generated_time_series_%i_%i'%(timestamp,epoch,index))\n",
    "                np.save('./npy/%s/generated_time_series_%i_%i.npy'%(timestamp,epoch,index),generated_series)\n",
    "            # update discriminator\n",
    "            X = np.concatenate((real_series, generated_series))\n",
    "            y = np.concatenate([np.random.uniform(0.9,1.1,batch_size),np.random.uniform(0.1,0.3,batch_size)])\n",
    "            d_loss = discriminator.train_on_batch(X, y)\n",
    "            d_loss_recorder.append(d_loss)\n",
    "            # update generator\n",
    "            y = np.array([1.]*batch_size,dtype=np.float32)\n",
    "            g_loss = gan.train_on_batch(noise, y)\n",
    "            g_loss_recorder.append(g_loss)\n",
    "            print(\"epoch: %d, batch: %d, g_loss: %f, d_loss: %f\" % (epoch, index, g_loss, d_loss))\n",
    "            generator.save_weights('./weights/%s/generator_%i_%i.h5'%(timestamp,epoch,index))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
